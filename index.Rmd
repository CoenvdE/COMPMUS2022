---
title: "Generations of soul"
author: "Coen van den Elsen"
data: "DATA"
output: 
  flexdashboard::flex_dashboard:
    storyboard: true
    orientation: columns
    vertical_layout: fill
    theme:
      version: 4
      bootswatch: minty
    self_contained: false
---

```{r setup, include=FALSE}
library(flexdashboard)
library(tidyverse)
library(spotifyr)
library(dplyr)
library(ggplot2)
library(plotly)
library(ggthemes)
library(ggridges)
library(viridis)
library(viridisLite)
library(forcats)
library(compmus)
library(cowplot)
library(gridExtra)
library(grid)
library(lattice)
library(tidymodels)
library(ggdendro)
library(heatmaply)
library(dendextend)
```


```{r data load generating_summary_statistics, echo = FALSE}

fkj <- get_playlist_audio_features("", "37i9dQZF1DZ06evO1x7AE9")
tom_misch <- get_playlist_audio_features("", "37i9dQZF1DZ06evO0P3UNG")
lianne_de_havas <- get_playlist_audio_features("", "37i9dQZF1DZ06evO1EwgdW")
joy_crookes <- get_playlist_audio_features("", "37i9dQZF1DZ06evO3w1CV3")
jorja_smith <- get_playlist_audio_features("", "37i9dQZF1DX1ykpeqTwA5m")
mahalia <- get_playlist_audio_features("", "37i9dQZF1DZ06evO0AHQ3T")

erykah_badu <- get_playlist_audio_features("", "37i9dQZF1DZ06evO4zhZAI")
steve_wonder <- get_playlist_audio_features("", "37i9dQZF1DZ06evO4iAGsg")
bill_withers <- get_playlist_audio_features("", "37i9dQZF1DX5MwHlrzAPLQ")
marvin_gaye <- get_playlist_audio_features("", "37i9dQZF1DZ06evO1VI5MY")
ray_charles <- get_playlist_audio_features("", "37i9dQZF1DZ06evO0FPX4A")
aretha_franklin <- get_playlist_audio_features("", "37i9dQZF1DZ06evO4mP172")

```


```{r data prep, echo = FALSE}
new <- rbind(fkj,tom_misch,lianne_de_havas,jorja_smith,mahalia,joy_crookes)
new <- new %>%
  add_column(generation = "new")

old <- rbind(erykah_badu,steve_wonder,bill_withers,marvin_gaye,ray_charles,aretha_franklin)
old <- old %>%
  add_column(generation = "old")

all <- rbind(old, new)

```  

```{r defining_a_theme, echo = FALSE}
# Better to define your own function than to always type the same stuff
theme_ilo <- function(){
  theme_minimal() +
  theme(
    text = element_text(color = "gray25"),
    plot.subtitle = element_text(size = 12),
    plot.caption = element_text(color = "gray30"),
    plot.background = element_rect(fill = "gray95"),
    plot.margin = unit(c(5, 10, 5, 10), units = "mm")
  )
}
```  

### **Introduction**

**Research Question**

*How has soul music changed over time?*

**Motivation**

For my project, I wanted to research soul music. I am a musician myself, I make soul/jazz/funk/RnB. I am curious how the soul music I listen and make everyday came to be 
and how the genre may differ between earlier and later generations. This is why I decided to see how big soul artists through the years compare to eachother. To decide what artists I should include, I made a mix of artists that inspire me, artists that have meant a lot for the genre in the perspective of my guitar teacher (who finished his jazz conservatory master) and artists that made the top according to the internet (Source 1). These are the artists per generation:

- **Old Generation**: Erykah Badu, Steve Wonder, Bill Withers, Marvin Gaye, Ray charles, Aretha Franklin.

- **new generation**: Tom Misch, Lianne De Havas, FKJ, Joy Crookes, Jorja Smith, Mahalia.

My guitar teacher, me, and the sources mentioned earlier, think these artists really represent the genre in their time and think it is a good representation of what I want to research. I do think more many names could be added, such as **Bobby Hebbs**, but I donâ€™t think this would be beneficial. The tracks of all these artists will be compared, I think **Bill Withers** is the most atypical, because he makes music in a lot of genres besides soul (like blues etc.).

**Assumptions**

I suspect the most difference will be seen in **tempo**, as most songs have been going faster and getting shorter through the years (Source 2). Besides that, I expect a change in **timbre**. Through the years, music has been getting more "electronic", with the upcoming of new instruments, like virtual synthesizers (Source 3). Because of better music production standards, compression, limiting and recording, I also expect music to be more **energetic** (louder) over the years (Source 4). Since all of the artists do represent the same genre, I think these differences won't be large, but noticeable.

*Source 1*: https://www.rollingstone.com/music/music-lists/rolling-stone-readers-pick-the-top-10-rb-soul-singers-of-all-time-16607/

*Source 2*: https://www.bbc.com/news/entertainment-arts-53167325

*Source 3*: https://www.theguardian.com/technology/2013/nov/25/pop-music-louder-less-acoustic

*Source 4*: https://en.wikipedia.org/wiki/Loudness_war


### Overview of the **new generation**

```{r fig.height = 6, fig.width = 8, fig.align = "center", warning=FALSE,  echo = FALSE}
ggplotly(
  ggplot(new,                         # Set up the plot.
    aes(
      x = valence,
      y = energy,
      size = track.popularity,
      colour = danceability,
      label = track.name           # Labels will be interactively visible.
    )  
  ) + theme_ilo() + theme_tufte() +
  geom_point(show.legend =TRUE) +                   # Scatter plot.
  geom_rug(size = 0.1) +           # Add 'fringes' to show data distribution.
  facet_wrap(~playlist_name) +           # Separate charts per country.
  scale_x_continuous(              # Fine-tune the x axis.
    limits = c(0, 1),
    breaks = c(0, 0.50, 1),        # Use grid-lines for quadrants only.
    minor_breaks = NULL            # Remove 'minor' grid-lines.
  ) +
  scale_y_continuous(              # Fine-tune the y axis in the same way.
    limits = c(0, 1),
    breaks = c(0, 0.50, 1),
    minor_breaks = NULL
  ) +
  scale_colour_viridis_c(          # Use the cividis palette
    option = "E",                  # Qualitative set.
    alpha = 0.8,                   # Include some transparency
  ) +
  scale_size_continuous(           # Fine-tune the sizes of each point.
  guide = "legend") +
  theme_light() +                  # Use a simpler theme.
  labs(                            # Make the titles nice.
    x = "Valence",
    y = "Energy"
  )
)

```

*** 

In the graphs, the new generation data is visualized. **Energy, valence, danceability and popularity** are displayed per track of every playlist/artist used for this generation. 

**Interesting observations**

There was an **outlier**, *"Silly Girl - Track Chat*, in the *This is Mahalia* playlist. This song had **no valence and danceability** stats, they are set to 0. Spotify has taken it out during the time I worked on the project. I Think this was a good decision, since it was a *track chat*, a discussion about a track she made, and not an actual song. I wanted to mention this, as it is possible for Spotify to put it back in, and in case that happens, I want to be able to explain it. Besides the outlier, there are some tracks in *This is Jorja Smith* playlist with a **popularity** of *zero*. I am not sure how this can be possible since they where listened to and it is unclear how Spotify determines the **popularity** of a track.

***

<iframe style="border-radius:12px" src="https://open.spotify.com/embed/playlist/37i9dQZF1DZ06evO0AHQ3T?utm_source=generator" width="100%" height="80" frameBorder="0" allowfullscreen="" allow="autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture"></iframe>

<iframe style="border-radius:12px" src="https://open.spotify.com/embed/playlist/37i9dQZF1DZ06evO1x7AE9?utm_source=generator" width="100%" height="80" frameBorder="0" allowfullscreen="" allow="autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture"></iframe>

<iframe style="border-radius:12px" src="https://open.spotify.com/embed/playlist/37i9dQZF1DZ06evO0P3UNG?utm_source=generator" width="100%" height="80" frameBorder="0" allowfullscreen="" allow="autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture"></iframe>

<iframe style="border-radius:12px" src="https://open.spotify.com/embed/playlist/37i9dQZF1DX1ykpeqTwA5m?utm_source=generator" width="100%" height="80" frameBorder="0" allowfullscreen="" allow="autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture"></iframe>

<iframe style="border-radius:12px" src="https://open.spotify.com/embed/playlist/37i9dQZF1DZ06evO3w1CV3?utm_source=generator" width="100%" height="80" frameBorder="0" allowfullscreen="" allow="autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture"></iframe>

<iframe style="border-radius:12px" src="https://open.spotify.com/embed/playlist/37i9dQZF1DZ06evO1EwgdW?utm_source=generator" width="100%" height="80" frameBorder="0" allowfullscreen="" allow="autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture"></iframe>

### Overview of the **old generation**

```{r fig.height = 6, fig.width = 8, fig.align = "center", warning=FALSE,  echo = FALSE}
ggplotly(
  ggplot(old,                         # Set up the plot.
    aes(
      x = valence,
      y = energy,
      size = track.popularity,
      colour = danceability,
      label = track.name           # Labels will be interactively visible.
    )
  ) + theme_ilo() + theme_tufte() +
  geom_point(show.legend = TRUE) +                   # Scatter plot.
  geom_rug(size = 0.1) +           # Add 'fringes' to show data distribution.
  facet_wrap(~playlist_name) +           # Separate charts per country.
  scale_x_continuous(              # Fine-tune the x axis.
    limits = c(0, 1),
    breaks = c(0, 0.50, 1),        # Use grid-lines for quadrants only.
    minor_breaks = NULL            # Remove 'minor' grid-lines.
  ) +
  scale_y_continuous(              # Fine-tune the y axis in the same way.
    limits = c(0, 1),
    breaks = c(0, 0.50, 1),
    minor_breaks = NULL
  ) +
  scale_colour_viridis_c(          # Use the cividis palette
    option = "E",                  # Qualitative set.
    alpha = 0.8,                   # Include some transparency
  ) +
  scale_size_continuous(           # Fine-tune the sizes of each point.
  guide = "legend") +
  theme_light() +                  # Use a simpler theme.
  labs(                            # Make the titles nice.
    x = "Valence",
    y = "Energy"
  )
)

```

***

In the graphs, the old generation data is visualized. **Energy, valence, danceability and popularity** are displayed per track of every playlist/artist used for this generation. 

**Interesting observations**

What is interesting to see is that there is one **outlier**, a song of *Erykah Badu*. This one is called *"Hi"*, which is not a "regular" song, but definitely music. It is in the *"This is Erykah Badu"* playlist. This song has **no  valence and danceability** stats, they are set to 0. Another interesting **outlier** is a song of *Bill Withers, "I Don't Want You On My Mind"*, which has a **very high tempo** (240-250). 

*** 

<iframe style="border-radius:12px" src="https://open.spotify.com/embed/playlist/37i9dQZF1DZ06evO4mP172?utm_source=generator" width="100%" height="80" frameBorder="0" allowfullscreen="" allow="autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture"></iframe>

<iframe style="border-radius:12px" src="https://open.spotify.com/embed/playlist/37i9dQZF1DZ06evO4zhZAI?utm_source=generator" width="100%" height="80" frameBorder="0" allowfullscreen="" allow="autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture"></iframe>

<iframe style="border-radius:12px" src="https://open.spotify.com/embed/playlist/37i9dQZF1DZ06evO0FPX4A?utm_source=generator" width="100%" height="80" frameBorder="0" allowfullscreen="" allow="autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture"></iframe>

<iframe style="border-radius:12px" src="https://open.spotify.com/embed/playlist/37i9dQZF1DX5MwHlrzAPLQ?utm_source=generator" width="100%" height="80" frameBorder="0" allowfullscreen="" allow="autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture"></iframe>

<iframe style="border-radius:12px" src="https://open.spotify.com/embed/playlist/37i9dQZF1DZ06evO1VI5MY?utm_source=generator" width="100%" height="80" frameBorder="0" allowfullscreen="" allow="autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture"></iframe>

<iframe style="border-radius:12px" src="https://open.spotify.com/embed/playlist/37i9dQZF1DZ06evO4iAGsg?utm_source=generator" width="100%" height="80" frameBorder="0" allowfullscreen="" allow="autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture"></iframe>

### What is the "actual" **tempo** of the **tempo-outlier** in the old generation?

```{r fig.height = 6, fig.width = 8, fig.align = "center", warning=FALSE,  echo = FALSE}
tempo_outlier <-
  get_tidy_audio_analysis("3BaFRtmUWO1Z3gzc9zYRh2")

outlier_graph1 <-
  tempo_outlier %>%
  tempogram(window_size = 8, hop_size = 1, cyclic = TRUE) %>%
  ggplot(aes(x = time, y = bpm, fill = power)) +
  geom_raster() +
  scale_fill_viridis_c(guide = "none") +
  labs(x = "Time (s)", y = "Tempo (BPM)") +
  ggtitle("Cyclic tempogram of the outlier", subtitle = "Bill Withers, I Don't Want You On My Mind") +
  theme_ilo() + theme_tufte()

outlier_graph2 <-
  tempo_outlier %>%
  tempogram(window_size = 8, hop_size = 1, cyclic = FALSE) %>%
  ggplot(aes(x = time, y = bpm, fill = power)) +
  geom_raster() +
  scale_fill_viridis_c(guide = "none") +
  labs(x = "Time (s)", y = "Tempo (BPM)") + 
  ggtitle("Regular tempogram of the outlier", subtitle = "Bill Withers, I Don't Want You On My Mind") +
  theme_ilo() + theme_tufte()

grid.arrange(outlier_graph1, outlier_graph2, ncol = 2)
```

***

The **outlier** track of *Bill Withers, "I Don't Want You On My Mind"* has a **very high tempo** (240-250). I wanted to see how this can be possible, since it is extraordinary to have a track with such high bpm.
For the explanation of the tempo outlier, two *Fourier tempograms* were created. These measure tempo based on onsets of Spotify segments. A cyclic tempogram (left) and a regular tempogram (right) visualize how tempo is analyzed. We created the cyclic tempogram as well, because the regular tempogram tends to pick up strongly on tempo harmonics. Wrapping these into a cyclic tempogram can be more informative. As we can see in the regular tempogram, the brightest line around the tempo of 220-250 bpm. In the non-cyclic graph, we can see the brightest line is is around **120 bpm**. This is the "actual" tempo of the track. Spotify measures the tempo to high, because of the tempo harmonics. 

***

<iframe style="border-radius:12px" src="https://open.spotify.com/embed/track/3BaFRtmUWO1Z3gzc9zYRh2?utm_source=generator" width="100%" height="80" frameBorder="0" allowfullscreen="" allow="autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture"></iframe>

### How do the generations compare in **danceability, valence, tempo and energy**?

```{r fig.height = 6, fig.width = 8, fig.align = "center", warning=FALSE,  echo = FALSE}
danceability <- 
  ggplot(all, aes(x = danceability, y = generation, fill = generation)) +
  geom_density_ridges(
    aes(point_fill = generation, point_size = valence), 
    alpha = .6, point_alpha = 0.1, jittered_points = FALSE
  ) + theme_tufte() + theme_ilo() + ggtitle("Danceability")

energy <-
  ggplot(all, aes(x = energy, y = generation, fill = generation)) +
  geom_density_ridges(
    aes(point_fill = generation, point_size = valence), 
    alpha = .6, point_alpha = 0.1, jittered_points = FALSE
  ) + theme_tufte() + theme_ilo() + ggtitle("Energy")

tempo <- 
  ggplot(all, aes(x = tempo, y = generation, fill = generation)) +
  geom_density_ridges(
    aes(point_fill = generation, point_size = valence), 
    alpha = .6, point_alpha = 0.1, jittered_points = FALSE
  ) + theme_tufte() + theme_ilo() + ggtitle("Tempo")

valence <-
  ggplot(all, aes(x = valence, y = generation, fill = generation)) +
  geom_density_ridges(
    aes(point_fill = generation, point_size = valence), 
    alpha = .6, point_alpha = 0.1, jittered_points = FALSE
  ) + theme_tufte() + theme_ilo() + ggtitle("Valence")

grid.arrange(danceability, energy, tempo, valence, ncol = 2)
```

*** 

To see how the generations compare, some major features are visualized As can be seen, the *old generation* generally makes songs that are **higher in tempo and valence** then the *new generation*. The higher tempo was surprising, since this means that the genre does not follow the trend of speeding up its songs.
Besides that, the *new generation* generally makes songs that have **higher danceability**, but **lower energy** than the *old generation*. The lower energy was surprising as well, since the genre does not follow the trend of louder/more energetic music as well. I think it is interesting that the **tempo** goes down, but the **danceability** goes up. This could imply that songs are getting **groovier**.

### How do the generations compare in low level **timbre coefficients**?

```{r}

new <- rbind(fkj,tom_misch,lianne_de_havas,jorja_smith,mahalia,joy_crookes)
new <- new %>%
  add_column(generation = "new")

old <- rbind(erykah_badu,steve_wonder,bill_withers,marvin_gaye,ray_charles,aretha_franklin)
old <- old %>%
  add_column(generation = "old")

all <- rbind(old, new)
old_gen_comp <-
  old %>%
  slice(1:30) %>%
  add_audio_analysis()

new_gen_comp <-
  new %>%
  slice(1:30) %>%
  add_audio_analysis()

all_gen_comp <-
  old_gen_comp %>%
  mutate(generation = "old") %>%
  bind_rows(new_gen_comp %>% mutate(generation = "new"))

genplt <-
  all_gen_comp %>%
  mutate(
    timbre =
      map(
        segments,
        compmus_summarise,
        timbre,
        method = "mean"
      )
  ) %>%
  select(generation, timbre) %>%
  compmus_gather_timbre() %>%
  ggplot(aes(x = basis, y = value, fill = generation)) +
  geom_violin() +
  scale_fill_viridis_d() +
  labs(x = "Spotify Timbre Coefficients", y = "", fill = "Generation", 
       title = "Comparison of Spotify Timbre coefficients") +
  theme_ilo() + theme_tufte()


genplt

```

***

**Timbre** is can be described as the *feel* of a track, or in this case a generation. While the term is hard to describe, this can be a nice way of summarizing what it means. 

Because it is interesting to see if the *feel* of the generations has changed, lower level timbre coefficients are compared. We expected this to differ noticeably, since music has been getting more electronic, but not drastically, since the tracks are from the same genre.
As we can see, **most timbre coefficients do not differ a lot between the generations**, but there are a few noticeable ones: **2, 3 and 4**. This could mean that the feel of the tracks do not have changed a lot in general, but that there are some slight differences. This could be explained by the fact that the tracks are of the same genre, thus all having a similar *soul* feeling. This matches with our assumption.

### Comparing an original with a cover with **Dynamic Time Warping**

```{r fig.height = 6, fig.width = 8, fig.align = "center", warning=FALSE,  echo = FALSE}

circshift <- function(v, n) {
if (n == 0) v else c(tail(v, n), head(v, -n))
}

transpose_pitch = function(pitch_list, number_of_semitones) { # n = 1: C -> C#
names = names(pitch_list)
new_list = setNames(circshift(unname(pitch_list), number_of_semitones), names)
new_list
}

transpose_pitches = function(df, n) { # n = 1 is C -> C#
df %>% dplyr::mutate(pitches = purrr::map2(pitches, n, transpose_pitch))
}

# E min
DTW_cover <-
  get_tidy_audio_analysis("1ZczOoLuCyDO5dKUPndxf5") %>%
  select(segments) %>%
  unnest(segments) %>%
  select(start, duration, pitches)

DTW_cover_transposed = transpose_pitches(DTW_cover, 2)

# A maj - F# min
DTW_original <-
  get_tidy_audio_analysis("3NfxSdJnVdon1axzloJgba") %>%
  select(segments) %>%
  unnest(segments) %>%
  select(start, duration, pitches)

compmus_long_distance(
  DTW_original %>% mutate(pitches = map(pitches, compmus_normalise, "chebyshev")),
  DTW_cover_transposed %>% mutate(pitches = map(pitches, compmus_normalise, "chebyshev")),
  feature = pitches,
  method = "euclidean"
) %>%
  ggplot(
    aes(
      x = xstart + xduration / 2,
      width = xduration,
      y = ystart + yduration / 2,
      height = yduration,
      fill = d
    )
  ) +
  geom_tile() +
  coord_equal() +
  labs(x = "Cover by Lianne La Havas (transposed)", y = "Original by Aretha Franklin", title = "Dynamic Time Warping of \"Say a little prayer\"") +
  theme_minimal() +
  scale_fill_viridis_c(guide = NULL) + theme_ilo() + theme_tufte()
```

*** 

Another interesting point to see if (and how) the generations differ from eachother, is to **compare the cover of a song from the old generation, by an artist of the new generation**.
In the Dynamic Time Warping Matrix, *Say a little prayer* by *Aretha Franklin* is compared to the *cover* of *Lianne de Havas*. To compare the two songs, they need to be in the same key, as *Dynamic time warping* compares *pith* information. The cover was in Emin and the original in F#min, so the cover was transposed two semitones up to F#min.

As we can see in the visualization there is **no diagonal line that shows similarities**. This is interesting because it is a cover of the same song, compared in the same key. The absence of similarities can have multiple reasons. The first is that the instrumentals are different. Besides that, the guitar in the cover version is very upfront and it played with a lot of vibrato which could throw the pitch information off. Lastly, the cover is a live performance and is sang in a different way than the original (which is not a live performance). It is interesting to see that the songs do not compare and all, which could mean that the new generation of soul has a very different way of expressing the genre than the old generation. However this is **very** generalized, as we only compare one cover here.

***

<iframe style="border-radius:12px" src="https://open.spotify.com/embed/track/3NfxSdJnVdon1axzloJgba?utm_source=generator" width="100%" height="80" frameBorder="0" allowfullscreen="" allow="autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture"></iframe>

<iframe style="border-radius:12px" src="https://open.spotify.com/embed/track/1ZczOoLuCyDO5dKUPndxf5?utm_source=generator" width="100%" height="80" frameBorder="0" allowfullscreen="" allow="autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture"></iframe>

### How the artists actually compare to eachother with **Clustering**?

```{r}
# set up classification
get_conf_mat <- function(fit) {
  outcome <- .get_tune_outcome_names(fit)
  fit %>% 
    collect_predictions() %>% 
    conf_mat(truth = outcome, estimate = .pred_class)
}  

get_pr <- function(fit) {
  fit %>% 
    conf_mat_resampled() %>% 
    group_by(Prediction) %>% mutate(precision = Freq / sum(Freq)) %>% 
    group_by(Truth) %>% mutate(recall = Freq / sum(Freq)) %>% 
    ungroup() %>% filter(Prediction == Truth) %>% 
    select(class = Prediction, precision, recall)
}  

# set up playlist
artists_all <-
  all%>%
  add_audio_analysis() %>%
  mutate(
    segments = map2(segments, key, compmus_c_transpose),
    pitches =
      map(segments,
          compmus_summarise, pitches,
          method = "mean", norm = "manhattan"
      ),
    timbre =
      map(
        segments,
        compmus_summarise, timbre,
        method = "mean"
      )
  ) %>%
  mutate(pitches = map(pitches, compmus_normalise, "clr")) %>%
  mutate_at(vars(pitches, timbre), map, bind_rows) %>%
  unnest(cols = c(pitches, timbre))

artists_all_filter <- artists_all %>% group_by(playlist_name) %>%
  summarise_at(vars(danceability, energy, speechiness, acousticness, instrumentalness, valence, tempo, c04, c03, c02), mean)

artists_all_filter_juice <-
  recipe(
    playlist_name ~
      danceability +
      energy +
      speechiness +
      acousticness +
      instrumentalness +
      valence +
      tempo +
      c02 + c04 + c03,
    data = artists_all_filter
  ) %>%
  step_center(all_predictors()) %>%
  step_scale(all_predictors()) %>% 
  prep(artists_all_filter %>% mutate(playlist_name = substring(playlist_name, 8))) %>%
  juice() %>%
  column_to_rownames("playlist_name")


artists_all_filter_dist <- dist(artists_all_filter_juice, method = "euclidean")

hc_all <- hclust(artists_all_filter_dist, method = "average")
dend <- as.dendrogram(hc_all)
dend <- dend %>%
  color_branches(k = 12) %>%
  set("branches_lwd", c(2))
  par(mfrow = c(1,1))
dend <- color_labels(dend, k = 12)
plot(dend)

```

***

To see if the artists from the generation actually represent their own generation, clustering was performed. If the generations are grouped clearly in two clusters, there would be less overlap than if they are not. To perform this clustering, the average of all songs per artist was taken, to get an "average overview" per artist. This is based on the features that we discussed earlier in the portfolio: **danceability, energy, speechiness, acousticness, instrumentalness, valence, tempo, c02, c03, c04** (the last two are low level timbre components). On this averaged data per artist, clustering is performed. For this, average, complete and single clustering linkage methods where tried, of which average gave the best (and only usable) result. What these methods do exactly, is too technical to discuss in this portfolio. The result of the clustering is discussed below.

As we can see, **the clustering is not perfect**. At the first split, FKJ and Tom Misch are clustered. These artists are quite similar and of the new generation, so it makes sense. The last "big" cluster that is split off contains Bill Withers, Aretha Franklin, Marvin Gaye and Stevie Wonder, all artists from the old generation. The two splits in between are more mixed. Ray Charles and Erykah Badu are mixed in with the clusters of the new generation here. Besides these two artists, the clustering is pretty good. Four artists of the old generation are in the right cluster, and 2 artists of the new generation in the left cluster. This could mean that there is a pretty good parting of music of the old and new generation, but that Erykah Badu and Ray Charles made music that is *"timeless"*. Because everyone of the old generation is clustered at the last "major" split, apart from them. They are mixed in with the clusters from the new generation.

### What did we **learn** and what could be **improved**?

So, what can we **learn from the analyses**?

We have analysed, visualized and described a lot in this portfolio, so what did we learn?
First of all, when comparing the old and new generation of soul with the chosen artists,
we learned that the **energy, valence, danceability and tempo** between generations were different.
Sometimes it were minor changes, other times more significant. 
It is possible that the new generation makes groovier music, as the tempo went down and danceability went up,
comparing to the old generation. Our initial assumptions about the features were wrong, the soul genre does not follow the trend of getting higher in tempo and energy. One explanation could be that the trends are set by pop music, which soul is not (yet).
When comparing the low level timbre components, the generations did not differ much, except for three components. This corresponds with our assumptions: because the tracks represent the same genre, the timbre did not differ much, but noticeably, since new instruments are used. Lastly, we saw that clustering the artists went fairly well, most artists from the old and new generation where clustered together, except for **Erykah Badu** and **Ray Charles**. This could be because their music is more *timeless* than the music of other artists from the old generation, resulting in a "clustering blend" between old and new generation in their case. Some of our initial assumptions where **wrong**, while others were **correct**. We can conclude that **the genre of soul did change over time, but not drastically**.

What could be improved in **future research?**

In the portfolio, the old and new generation of soul artists exist of 6 (arbitrarily chosen) artists. While this gives some representation of how the generations compare, for further research it is adviced to use larger artist groups, with artists chosen carefully. Furthermore, the outliers in the generations were not taken out in this research, this is because Spotify keeps putting them in and taking them out. This means sometimes the outliers are in the data and other times they are not. To improve this, the outliers should be removed at the time they are both in there, after which the data is saved to work with in the future. Another point is that the comparison of an original with a cover, only consists of *one* cover. This is a very small number of comparisons, and needs to be bigger in future research. Lastly, the **popularity** feature is vague. The Spotify API does not explain how this feature is measured, or why there is suddenly outliers with 0 popularity, like we saw in the new generation. For further research, it would be interesting to see how the **popularity** of soul changes over time as well, since soul elements appear in pop music more often. For this research, it needs to be clear how the popularity feature is measured.

### Appendix: What does **transposing** look like?

```{r fig.height = 6, fig.width = 8, fig.align = "center", warning=FALSE,  echo = FALSE}

sunnyp <-
  get_tidy_audio_analysis("6IFSPx3lqkw0ri4OJkTkLl") %>%
  select(segments) %>%
  unnest(segments) %>%
  select(start, duration, pitches)


sunny <-
  sunnyp %>%
  mutate(pitches = map(pitches, compmus_normalise, "chebyshev")) %>%
  compmus_gather_chroma() %>% 
  ggplot(
    aes(
      x = start + duration / 2,
      width = duration,
      y = pitch_class,
      fill = value
    )
  ) +
  geom_tile() +
  labs(x = "Time (s)", y = NULL, fill = "Magnitude", title = "Chromagram of \"Sunny\"", subtitle = "Transposing from Fmin to G#min in steps") +
  theme_minimal() +
  scale_fill_viridis_c() +
  theme_ilo() + theme_tufte()

circshift <- function(v, n) {
  if (n == 0) v else c(tail(v, n), head(v, -n))
}

#      C     C#    D     Eb    E     F     F#    G     Ab    A     Bb    B
major_chord <-
  c(   1,    0,    0,    0,    1,    0,    0,    1,    0,    0,    0,    0)
minor_chord <-
  c(   1,    0,    0,    1,    0,    0,    0,    1,    0,    0,    0,    0)
seventh_chord <-
  c(   1,    0,    0,    0,    1,    0,    0,    1,    0,    0,    1,    0)

major_key <-
  c(6.35, 2.23, 3.48, 2.33, 4.38, 4.09, 2.52, 5.19, 2.39, 3.66, 2.29, 2.88)
minor_key <-
  c(6.33, 2.68, 3.52, 5.38, 2.60, 3.53, 2.54, 4.75, 3.98, 2.69, 3.34, 3.17)



chord_templates <-
  tribble(
    ~name, ~template,
    "Gb:7", circshift(seventh_chord, 6),
    "Gb:maj", circshift(major_chord, 6),
    "Bb:min", circshift(minor_chord, 10),
    "Db:maj", circshift(major_chord, 1),
    "F:min", circshift(minor_chord, 5),
    "Ab:7", circshift(seventh_chord, 8),
    "Ab:maj", circshift(major_chord, 8),
    "C:min", circshift(minor_chord, 0),
    "Eb:7", circshift(seventh_chord, 3),
    "Eb:maj", circshift(major_chord, 3),
    "G:min", circshift(minor_chord, 7),
    "Bb:7", circshift(seventh_chord, 10),
    "Bb:maj", circshift(major_chord, 10),
    "D:min", circshift(minor_chord, 2),
    "F:7", circshift(seventh_chord, 5),
    "F:maj", circshift(major_chord, 5),
    "A:min", circshift(minor_chord, 9),
    "C:7", circshift(seventh_chord, 0),
    "C:maj", circshift(major_chord, 0),
    "E:min", circshift(minor_chord, 4),
    "G:7", circshift(seventh_chord, 7),
    "G:maj", circshift(major_chord, 7),
    "B:min", circshift(minor_chord, 11),
    "D:7", circshift(seventh_chord, 2),
    "D:maj", circshift(major_chord, 2),
    "F#:min", circshift(minor_chord, 6),
    "A:7", circshift(seventh_chord, 9),
    "A:maj", circshift(major_chord, 9),
    "C#:min", circshift(minor_chord, 1),
    "E:7", circshift(seventh_chord, 4),
    "E:maj", circshift(major_chord, 4),
    "G#:min", circshift(minor_chord, 8),
    "B:7", circshift(seventh_chord, 11),
    "B:maj", circshift(major_chord, 11),
    "D#:min", circshift(minor_chord, 3)
  )

key_templates <-
  tribble(
    ~name, ~template,
    "Gb:maj", circshift(major_key, 6),
    "Bb:min", circshift(minor_key, 10),
    "Db:maj", circshift(major_key, 1),
    "F:min", circshift(minor_key, 5),
    "Ab:maj", circshift(major_key, 8),
    "C:min", circshift(minor_key, 0),
    "Eb:maj", circshift(major_key, 3),
    "G:min", circshift(minor_key, 7),
    "Bb:maj", circshift(major_key, 10),
    "D:min", circshift(minor_key, 2),
    "F:maj", circshift(major_key, 5),
    "A:min", circshift(minor_key, 9),
    "C:maj", circshift(major_key, 0),
    "E:min", circshift(minor_key, 4),
    "G:maj", circshift(major_key, 7),
    "B:min", circshift(minor_key, 11),
    "D:maj", circshift(major_key, 2),
    "F#:min", circshift(minor_key, 6),
    "A:maj", circshift(major_key, 9),
    "C#:min", circshift(minor_key, 1),
    "E:maj", circshift(major_key, 4),
    "G#:min", circshift(minor_key, 8),
    "B:maj", circshift(major_key, 11),
    "D#:min", circshift(minor_key, 3)
  )

sunny10 <-
  get_tidy_audio_analysis("6IFSPx3lqkw0ri4OJkTkLl") %>%
  compmus_align(sections, segments) %>%
  select(sections) %>%
  unnest(sections) %>%
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "mean", norm = "manhattan"
      )
  )

sunny1 <-
  sunny10 %>% 
  compmus_match_pitch_template(
    key_templates,         # Change to chord_templates if descired
    method = "euclidean",  # Try different distance metrics
    norm = "manhattan"     # Try different norms
  ) %>%
  ggplot(
    aes(x = start + duration / 2, width = duration, y = name, fill = d)
  ) +
  geom_tile() +
  scale_fill_viridis_c(guide = "none") +
  theme_minimal() +
  labs(x = "Time (s)", y = "", title = "Keygram of \"Sunny\"", subtitle = "Transposing 3x") +
  theme_ilo() + theme_tufte()

sunny2 <-
  sunny10 %>% 
  compmus_match_pitch_template(
    chord_templates,         # Change to chord_templates if descired
    method = "euclidean",  # Try different distance metrics
    norm = "manhattan"     # Try different norms
  ) %>%
  ggplot(
    aes(x = start + duration / 2, width = duration, y = name, fill = d)
  ) +
  geom_tile() +
  scale_fill_viridis_c(guide = "none") +
  theme_minimal() +
  labs(x = "Time (s)", y = "", title = "Chordogram of \"Sunny\"", subtitle = "Transposing 3x") +
  theme_ilo() + theme_tufte()

grid.arrange(sunny1, sunny, ncol = 2, widths = 1:2)
```

***

I decided to visualize the song *"Sunny"* by *Marvin Gaye* (the original is from Bobby Hebbs).
I wanted to visualize this song, because it is a great song in itself, and **transposes** a few times. As seen in the chromogram, the song starts in *Fmin* transposes to *F#min*, *Gmin* and ends in *G#min*. We can see this in our keyogram as well. I think this song is very interesting to see in the visualization, as you can *"see the transposing happen"*. 

***

<iframe style="border-radius:12px" src="https://open.spotify.com/embed/track/6IFSPx3lqkw0ri4OJkTkLl?utm_source=generator&theme=0" width="100%" height="80" frameBorder="0" allowfullscreen="" allow="autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture"></iframe>

<iframe style="border-radius:12px" src="https://open.spotify.com/embed/track/4vju55Ag7apDL2CfotuE7Q?utm_source=generator" width="100%" height="80" frameBorder="0" allowfullscreen="" allow="autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture"></iframe>

### Appendix: How can we describe the **structure** of a track?

```{r fig.height = 6, fig.width = 8, fig.align = "center", warning=FALSE,  echo = FALSE}
tom_misch_self_similar <-   
  get_tidy_audio_analysis("4yMT3mbUsiukLDRqPfQ9rN") %>% # Change URI.
  compmus_align(bars, segments) %>%                     # Change `bars`
  select(bars) %>%                                      #   in all three
  unnest(bars) %>%                                      #   of these lines.
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "rms", norm = "euclidean"              # Change summary & norm.
      )
  ) %>%
  mutate(
    timbre =
      map(segments,
        compmus_summarise, timbre,
        method = "rms", norm = "euclidean"              # Change summary & norm.
      )
  )

SS1 <- tom_misch_self_similar %>%
  compmus_self_similarity(timbre, "cosine") %>% 
  ggplot(
    aes(
      x = xstart + xduration / 2,
      width = xduration,
      y = ystart + yduration / 2,
      height = yduration,
      fill = d
    )
  ) +
  geom_tile() +
  coord_fixed() +
  scale_fill_viridis_c(guide = "none") +
  theme_classic() +
  labs(x = "", y = "")  + ggtitle("Timbre SSM of \"Losing my way\"", subtitle = "sections") + 
  theme_ilo() + theme_tufte()

SS2 <- tom_misch_self_similar %>%
  compmus_self_similarity(pitches, "cosine") %>% 
  ggplot(
    aes(
      x = xstart + xduration / 2,
      width = xduration,
      y = ystart + yduration / 2,
      height = yduration,
      fill = d
    )
  ) +
  geom_tile() +
  coord_fixed() +
  scale_fill_viridis_c(guide = "none") +
  theme_classic() +
  labs(x = "", y = "") + ggtitle("Chroma SSM of \"Losing my way\"", subtitle = "sections") + 
  theme_ilo() + theme_tufte()

grid.arrange(SS1, SS2, ncol = 2)

```

*** 

To understand how the structure of a track works, a track of the new generation is visualized with *Self Similarity matrices (SSM)*. The track is made by two artists from the new generation, making it more representable for the generation than other songs. In the Self-Similarity matrices, the track *Losing my way* from *Tom Misch & FKJ* is analysed.

First we analyse via a **timbre SSM**. **Timbre** can be described as the feel of a track, and what different instruments are used, as mentioned earlier. As the **Timbre SSM shows**, there are different sections in the song, of which some repeat as well. The sections can be described as following: A - B - C - A - (B+C) - D. 

The **chroma SSM** shows how **pitch** characteristics of a song relate to eachother. As we can see in the chroma SSM, different segments of the song have different pitch characteristics. Light means the pitch characteristics are *very different* from the rest of the track. This is the case for the **intro, bridge and outro** of the track. The rest of the pattern follows the sections, as described above, quite well. The colors in the rest of the matrix are darker, even though the pitch characteristics differentiate during the rest of the song. This is because compared to the intro, bridge and outro, the pitch characteristics of the rest of the track are quite similar.

***

<iframe style="border-radius:12px" src="https://open.spotify.com/embed/track/4yMT3mbUsiukLDRqPfQ9rN?utm_source=generator" width="100%" height="80" frameBorder="0" allowfullscreen="" allow="autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture"></iframe>

