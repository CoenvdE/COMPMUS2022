---
title: "Portfolio"
author: "Coen van den Elsen"
data: "DATA"
output: 
  flexdashboard::flex_dashboard:
    storyboard: true
    orientation: columns
    vertical_layout: fill
    theme:
      version: 4
      bootswatch: minty
    self_contained: false
---

```{r setup, include=FALSE}
library(flexdashboard)
library(tidyverse)
library(spotifyr)
library(dplyr)
library(ggplot2)
library(plotly)
library(ggthemes)
library(ggridges)
library(viridis)
library(viridisLite)
library(forcats)
library(compmus)
library(cowplot)
library(gridExtra)
library(grid)
library(lattice)
library(tidymodels)
library(ggdendro)
library(heatmaply)
```


```{r data load generating_summary_statistics, echo = FALSE}

fkj <- get_playlist_audio_features("", "37i9dQZF1DZ06evO1x7AE9")
tom_misch <- get_playlist_audio_features("", "37i9dQZF1DZ06evO0P3UNG")
lianne_de_havas <- get_playlist_audio_features("", "37i9dQZF1DZ06evO1EwgdW")
joy_crookes <- get_playlist_audio_features("", "37i9dQZF1DZ06evO3w1CV3")
jorja_smith <- get_playlist_audio_features("", "37i9dQZF1DX1ykpeqTwA5m")
mahalia <- get_playlist_audio_features("", "37i9dQZF1DZ06evO0AHQ3T")

erykah_badu <- get_playlist_audio_features("", "37i9dQZF1DZ06evO4zhZAI")
steve_wonder <- get_playlist_audio_features("", "37i9dQZF1DZ06evO4iAGsg")
bill_withers <- get_playlist_audio_features("", "37i9dQZF1DX5MwHlrzAPLQ")
marvin_gaye <- get_playlist_audio_features("", "37i9dQZF1DZ06evO1VI5MY")
ray_charles <- get_playlist_audio_features("", "37i9dQZF1DZ06evO0FPX4A")
aretha_franklin <- get_playlist_audio_features("", "37i9dQZF1DZ06evO4mP172")

```


```{r data prep, echo = FALSE}
new <- rbind(fkj,tom_misch,lianne_de_havas,jorja_smith,mahalia,joy_crookes)
new <- new %>%
  add_column(generation = "new")

old <- rbind(erykah_badu,steve_wonder,bill_withers,marvin_gaye,ray_charles,aretha_franklin)
old <- old %>%
  add_column(generation = "old")

all <- rbind(old, new)

```  

```{r defining_a_theme, echo = FALSE}
# Better to define your own function than to always type the same stuff
theme_ilo <- function(){
  theme_minimal() +
  theme(
    text = element_text(color = "gray25"),
    plot.subtitle = element_text(size = 12),
    plot.caption = element_text(color = "gray30"),
    plot.background = element_rect(fill = "gray95"),
    plot.margin = unit(c(5, 10, 5, 10), units = "mm")
  )
}
```  

### Clustering 

```{r}
# set up classification
get_conf_mat <- function(fit) {
  outcome <- .get_tune_outcome_names(fit)
  fit %>% 
    collect_predictions() %>% 
    conf_mat(truth = outcome, estimate = .pred_class)
}  

get_pr <- function(fit) {
  fit %>% 
    conf_mat_resampled() %>% 
    group_by(Prediction) %>% mutate(precision = Freq / sum(Freq)) %>% 
    group_by(Truth) %>% mutate(recall = Freq / sum(Freq)) %>% 
    ungroup() %>% filter(Prediction == Truth) %>% 
    select(class = Prediction, precision, recall)
}  

# set up playlist
fkj <-
  get_playlist_audio_features("", "37i9dQZF1DZ06evO1x7AE9") %>%
  add_audio_analysis() %>%
  mutate(
    segments = map2(segments, key, compmus_c_transpose),
    pitches =
      map(segments,
          compmus_summarise, pitches,
          method = "mean", norm = "manhattan"
      ),
    timbre =
      map(
        segments,
        compmus_summarise, timbre,
        method = "mean"
      )
  ) %>%
  mutate(pitches = map(pitches, compmus_normalise, "clr")) %>%
  mutate_at(vars(pitches, timbre), map, bind_rows) %>%
  unnest(cols = c(pitches, timbre))


fkj_juice <-
  recipe(
    track.name ~ #track artist RECAST AS FACTOR EN DAN <- factor(list, levels = c("name goeie orders moet precies"))
      danceability +
      energy +
      speechiness +
      acousticness +
      instrumentalness +
      valence +
      tempo +
      c01 + c02 + c03 + c04 + c05 + c06 +
      c07 + c08 + c09 + c10 + c11 + c12,
    data = fkj
  ) %>%
  step_center(all_predictors()) %>%
  step_scale(all_predictors()) %>% 
  # step_range(all_predictors()) %>% 
  prep(fkj %>% mutate(track.name = str_trunc(track.name, 20))) %>%
  juice() %>%
  column_to_rownames("track.name")


fkj_dist <- dist(fkj_juice, method = "euclidean")

cluster_ding <- fkj_dist %>% 
  hclust(method = "complete") %>% # Try single, average, and complete.
  dendro_data() %>%
  ggdendrogram()


heatmap_ding <- heatmaply(
  fkj_juice,
  hclustfun = hclust,
  hclust_method = "complete",  # Change for single, average, or complete linkage.
  dist_method = "euclidean"
)

cluster_ding
```

***

For now I just clustered the songs of FKJ, in the future i will hopefully be able to cluster all the artists of my corpus, but it requires some heavy data prep which i need to look into more still. For now it is interesting to see that it "perfectly" clusters the slower piano songs, and the rest.


### heatmap of clusters

```{r}
heatmap_ding

```

***

Here we can see what features are selected for clustering and what their role is. the color of the feature is how it compares to other tracks in the data

### Corpus

**Research Question**

*How has the soul changed over time? WHY*

**Motivation**

I want to research soul music. I am a musician myself and make soul/jazz/funk/RnB, I am curious how the music I listen and make everyday came to be and differce from earlier Thats why I decided to see how big soul artists through the years compare to eachother. To decide what artists I should include, I made a mix of artists that inspire me, artists that have ment a lot for the genre in the perspective of my guitar teacher (who finished his jazz conservatory master) and artists that made the top according to websites (Sources). These are the artists per generation:

- **Old Generation**: Erykah Badu, Steve Wonder, Bill Withers, Marvin Gaye, Ray charles, Aretha Franklin.

- **new generation**: Tom Misch, Lianne De Havas, FKJ, Joy Crookes, Jorja Smith, Mahalia.

I, and the sources mentioned earlier, think these artists really represent the genre in their time and think it is a good representation of what I want to research. I do think more many names could be added, such as **Bobby Hebbs**, but I donâ€™t think this would be of use. 

**Hypotheses**

I suspect the most difference will be seen in tempo and time, as most songs have been going faster and getting shorter through the years The tracks of all these artists will be compared, I think Bill Withers is the most atypical, because he does a lot of genres besides soul alone (like blues etc.)


*Source*: https://www.rollingstone.com/music/music-lists/rolling-stone-readers-pick-the-top-10-rb-soul-singers-of-all-time-16607/

### Overview of the **new generation**

```{r fig.height = 6, fig.width = 8, fig.align = "center", warning=FALSE,  echo = FALSE}
ggplotly(
  ggplot(new,                         # Set up the plot.
    aes(
      x = valence,
      y = energy,
      size = track.popularity,
      colour = danceability,
      label = track.name           # Labels will be interactively visible.
    )
  ) +
  geom_point(show.legend =TRUE) +                   # Scatter plot.
  geom_rug(size = 0.1) +           # Add 'fringes' to show data distribution.
  facet_wrap(~playlist_name) +           # Separate charts per country.
  scale_x_continuous(              # Fine-tune the x axis.
    limits = c(0, 1),
    breaks = c(0, 0.50, 1),        # Use grid-lines for quadrants only.
    minor_breaks = NULL            # Remove 'minor' grid-lines.
  ) +
  scale_y_continuous(              # Fine-tune the y axis in the same way.
    limits = c(0, 1),
    breaks = c(0, 0.50, 1),
    minor_breaks = NULL
  ) +
  scale_colour_viridis_c(          # Use the cividis palette
    option = "E",                  # Qualitative set.
    alpha = 0.8,                   # Include some transparency
  ) +
  scale_size_continuous(           # Fine-tune the sizes of each point.
  guide = "legend") +
  theme_light() +                  # Use a simpler theme.
  labs(                            # Make the titles nice.
    x = "Valence",
    y = "Energy"
  ) 
)

```

*** 

In the graphs, the new generation data is visualized. Energy, valence, danceability and popularity are displayed and per track of every playlist/artist used. 

Interesting observations

There was also an **outlier**, like *"Hi"* (see old generation overview), in the *This is Mahalia* playlist, Spotify has taken it out during the time I worked on the project. I Think this was a good decision, since it was a *track chat*, a discussion about a track she made, and not an actual song. I wanted to mention this, as it is possible for spotify to put it back in, and in case that happens, I want to be able to explain it. In *This is Jorja Smith*, there are also some tracks with a popularity of 0, I am not sure how this can be possible since they where listened to and it is unclear how spotify determines the popularity of a track.

***

<iframe style="border-radius:12px" src="https://open.spotify.com/embed/playlist/37i9dQZF1DZ06evO0AHQ3T?utm_source=generator" width="100%" height="80" frameBorder="0" allowfullscreen="" allow="autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture"></iframe>

<iframe style="border-radius:12px" src="https://open.spotify.com/embed/playlist/37i9dQZF1DZ06evO1x7AE9?utm_source=generator" width="100%" height="80" frameBorder="0" allowfullscreen="" allow="autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture"></iframe>

<iframe style="border-radius:12px" src="https://open.spotify.com/embed/playlist/37i9dQZF1DZ06evO0P3UNG?utm_source=generator" width="100%" height="80" frameBorder="0" allowfullscreen="" allow="autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture"></iframe>

<iframe style="border-radius:12px" src="https://open.spotify.com/embed/playlist/37i9dQZF1DX1ykpeqTwA5m?utm_source=generator" width="100%" height="80" frameBorder="0" allowfullscreen="" allow="autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture"></iframe>

<iframe style="border-radius:12px" src="https://open.spotify.com/embed/playlist/37i9dQZF1DZ06evO3w1CV3?utm_source=generator" width="100%" height="80" frameBorder="0" allowfullscreen="" allow="autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture"></iframe>

<iframe style="border-radius:12px" src="https://open.spotify.com/embed/playlist/37i9dQZF1DZ06evO1EwgdW?utm_source=generator" width="100%" height="80" frameBorder="0" allowfullscreen="" allow="autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture"></iframe>

### Overview of the **old generation**

```{r fig.height = 6, fig.width = 8, fig.align = "center", warning=FALSE,  echo = FALSE}
ggplotly(
  ggplot(old,                         # Set up the plot.
    aes(
      x = valence,
      y = energy,
      size = track.popularity,
      colour = danceability,
      label = track.name           # Labels will be interactively visible.
    )
  ) +
  geom_point(show.legend = TRUE) +                   # Scatter plot.
  geom_rug(size = 0.1) +           # Add 'fringes' to show data distribution.
  facet_wrap(~playlist_name) +           # Separate charts per country.
  scale_x_continuous(              # Fine-tune the x axis.
    limits = c(0, 1),
    breaks = c(0, 0.50, 1),        # Use grid-lines for quadrants only.
    minor_breaks = NULL            # Remove 'minor' grid-lines.
  ) +
  scale_y_continuous(              # Fine-tune the y axis in the same way.
    limits = c(0, 1),
    breaks = c(0, 0.50, 1),
    minor_breaks = NULL
  ) +
  scale_colour_viridis_c(          # Use the cividis palette
    option = "E",                  # Qualitative set.
    alpha = 0.8,                   # Include some transparency
  ) +
  scale_size_continuous(           # Fine-tune the sizes of each point.
  guide = "legend") +
  theme_light() +                  # Use a simpler theme.
  labs(                            # Make the titles nice.
    x = "Valence",
    y = "Energy"
  ) 
)

```

***

In the graphs, the old generation data is visualized. Energy, valence, danceability and popularity are displayed and per track of every playlist/artist used. 

Interesting observations

What is interesting to see is that there is one **outlier**, a song of *Erykah Badu*. This one is called *"Hi"*, which is not a full song, but definitely music, it is in the *"This is Erykah Badu"* playlist. This song has **no  valence and tempo** stats, they are set to 0. Another interesting point is that 1 song of *Bill Withers*,*Dont want you on my mind*, has a **very high tempo** (240-250).

*** 

<iframe style="border-radius:12px" src="https://open.spotify.com/embed/playlist/37i9dQZF1DZ06evO4mP172?utm_source=generator" width="100%" height="80" frameBorder="0" allowfullscreen="" allow="autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture"></iframe>

<iframe style="border-radius:12px" src="https://open.spotify.com/embed/playlist/37i9dQZF1DZ06evO4zhZAI?utm_source=generator" width="100%" height="80" frameBorder="0" allowfullscreen="" allow="autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture"></iframe>

<iframe style="border-radius:12px" src="https://open.spotify.com/embed/playlist/37i9dQZF1DZ06evO0FPX4A?utm_source=generator" width="100%" height="80" frameBorder="0" allowfullscreen="" allow="autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture"></iframe>

<iframe style="border-radius:12px" src="https://open.spotify.com/embed/playlist/37i9dQZF1DX5MwHlrzAPLQ?utm_source=generator" width="100%" height="80" frameBorder="0" allowfullscreen="" allow="autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture"></iframe>

<iframe style="border-radius:12px" src="https://open.spotify.com/embed/playlist/37i9dQZF1DZ06evO1VI5MY?utm_source=generator" width="100%" height="80" frameBorder="0" allowfullscreen="" allow="autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture"></iframe>

<iframe style="border-radius:12px" src="https://open.spotify.com/embed/playlist/37i9dQZF1DZ06evO4iAGsg?utm_source=generator" width="100%" height="80" frameBorder="0" allowfullscreen="" allow="autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture"></iframe>

### Taking a look at the **tempo** of the tempo-outlier in the old generation

```{r fig.height = 6, fig.width = 8, fig.align = "center", warning=FALSE,  echo = FALSE}
tempo_outlier <-
  get_tidy_audio_analysis("3BaFRtmUWO1Z3gzc9zYRh2")

outlier_graph1 <-
  tempo_outlier %>%
  tempogram(window_size = 8, hop_size = 1, cyclic = TRUE) %>%
  ggplot(aes(x = time, y = bpm, fill = power)) +
  geom_raster() +
  scale_fill_viridis_c(guide = "none") +
  labs(x = "Time (s)", y = "Tempo (BPM)") +
  theme_classic()

outlier_graph2 <-
  tempo_outlier %>%
  tempogram(window_size = 8, hop_size = 1, cyclic = FALSE) %>%
  ggplot(aes(x = time, y = bpm, fill = power)) +
  geom_raster() +
  scale_fill_viridis_c(guide = "none") +
  labs(x = "Time (s)", y = "Tempo (BPM)") +
  theme_classic()

grid.arrange(outlier_graph1, outlier_graph2, ncol = 2)
```

***

For the explanation of the tempo outlier, 2 tempograms were created. One cyclic and one normal, to see how the tempo would be analyzed. As we can see in the non-cyclic graph, we can see the brightest line around the tempo of 220-250 bpm. In the non-cyclic graph, we can see the brightest line is is around 120 bpm. This is the actual bpm. Spotify measures the tempo to high, a tempo octave to high even maybe. MEASURE IN ENERGY OR SPECTOGRAM, ENERGY = LOUDNESS
MORE MUSICOLOGICAL TERMS TOEVOEGEN EN OVERAL TITLES ENZO


### comparing the generations **danceability, valence, tempo and energy**

```{r fig.height = 6, fig.width = 8, fig.align = "center", warning=FALSE,  echo = FALSE}
danceability <- 
  ggplot(all, aes(x = danceability, y = generation, fill = generation)) +
  geom_density_ridges(
    aes(point_fill = generation, point_size = valence), 
    alpha = .6, point_alpha = 0.1, jittered_points = FALSE
  ) + theme_tufte() + theme_ilo() + ggtitle("Danceability")

energy <-
  ggplot(all, aes(x = energy, y = generation, fill = generation)) +
  geom_density_ridges(
    aes(point_fill = generation, point_size = valence), 
    alpha = .6, point_alpha = 0.1, jittered_points = FALSE
  ) + theme_tufte() + theme_ilo() + ggtitle("Energy")

tempo <- 
  ggplot(all, aes(x = tempo, y = generation, fill = generation)) +
  geom_density_ridges(
    aes(point_fill = generation, point_size = valence), 
    alpha = .6, point_alpha = 0.1, jittered_points = FALSE
  ) + theme_tufte() + theme_ilo() + ggtitle("Tempo")

valence <-
  ggplot(all, aes(x = valence, y = generation, fill = generation)) +
  geom_density_ridges(
    aes(point_fill = generation, point_size = valence), 
    alpha = .6, point_alpha = 0.1, jittered_points = FALSE
  ) + theme_tufte() + theme_ilo() + ggtitle("Valence")

grid.arrange(danceability, energy, tempo, valence, ncol = 2)
```

*** 

To see how the generations compare, some major characteristics are shown in graphs. As can be seen in the visualization, the *old generation* generally makes songs that are **higher in tempo and valence** then the *new generation*. This was surprising, since this means that the genre does not follow the trend of speeding up its songs.
Besides that, the *new generation* generally makes songs that have **higher danceability**, but **lower energy** than the *old generation*. I think it is interesting that the **tempo** goes down (as can be seen in the comparison of **tempo and valence**), but the **danceability** goes up. This could imply that songs are getting **groovier**.

### low level comparison between the generations (1)

```{r}
old_gen_comp <-
  old %>%
  slice(1:30) %>%
  add_audio_analysis()


new_gen_comp <-
  new %>%
  slice(1:30) %>%
  add_audio_analysis()

all_gen_comp <-
  old_gen_comp %>%
  mutate(generation = "old") %>%
  bind_rows(new_gen_comp %>% mutate(generation = "new"))

genplt1 <-
  all_gen_comp %>%
  mutate(
    sections =
      map(
        sections,                                    # sections or segments
        summarise_at,
        vars(tempo, loudness, duration),             # features of interest
        list(section_mean = mean, section_sd = sd)   # aggregation functions
      )
  ) %>%
  unnest(sections) %>%
  ggplot(
    aes(
      x = tempo,
      y = tempo_section_sd,
      colour = generation,
      alpha = loudness
    )
  ) +
  geom_point(aes(size = duration / 60)) +
  geom_rug() +
  theme_minimal() +
  ylim(0, 5) +
  labs(
    x = "Mean Tempo (bpm)",
    y = "SD Tempo",
    colour = "Generation",
    size = "Duration (min)",
    alpha = "Volume (dBFS)"
  )

genplt2 <-
  all_gen_comp %>%
  mutate(
    timbre =
      map(
        segments,
        compmus_summarise,
        timbre,
        method = "mean"
      )
  ) %>%
  select(generation, timbre) %>%
  compmus_gather_timbre() %>%
  ggplot(aes(x = basis, y = value, fill = generation)) +
  geom_violin() +
  scale_fill_viridis_d() +
  labs(x = "Spotify Timbre Coefficients", y = "", fill = "Generation")

genplt1

```

***
As we can see, most timbre coefficients do not differ a lot between the generations, except 2 and 4. This could mean that the feel of the tracks do not have changed a lot. This could be explained by presenting the same genre.

### low level comparison between the generations (2)

```{r}
old_gen_comp <-
  old %>%
  slice(1:30) %>%
  add_audio_analysis()


new_gen_comp <-
  new %>%
  slice(1:30) %>%
  add_audio_analysis()

all_gen_comp <-
  old_gen_comp %>%
  mutate(generation = "old") %>%
  bind_rows(new_gen_comp %>% mutate(generation = "new"))

genplt1 <-
  all_gen_comp %>%
  mutate(
    sections =
      map(
        sections,                                    # sections or segments
        summarise_at,
        vars(tempo, loudness, duration),             # features of interest
        list(section_mean = mean, section_sd = sd)   # aggregation functions
      )
  ) %>%
  unnest(sections) %>%
  ggplot(
    aes(
      x = tempo,
      y = tempo_section_sd,
      colour = generation,
      alpha = loudness
    )
  ) +
  geom_point(aes(size = duration / 60)) +
  geom_rug() +
  theme_minimal() +
  ylim(0, 5) +
  labs(
    x = "Mean Tempo (bpm)",
    y = "SD Tempo",
    colour = "Generation",
    size = "Duration (min)",
    alpha = "Volume (dBFS)"
  )

genplt2 <-
  all_gen_comp %>%
  mutate(
    timbre =
      map(
        segments,
        compmus_summarise,
        timbre,
        method = "mean"
      )
  ) %>%
  select(generation, timbre) %>%
  compmus_gather_timbre() %>%
  ggplot(aes(x = basis, y = value, fill = generation)) +
  geom_violin() +
  scale_fill_viridis_d() +
  labs(x = "Spotify Timbre Coefficients", y = "", fill = "Generation")


genplt2

```

***
As we can see, most timbre coefficients do not differ a lot between the generations, except 2 and 4. This could mean that the feel of the tracks do not have changed a lot. This could be explained by presenting the same genre.

### comparing an original with a cover from the generations with **Dynamic time wraping**

```{r fig.height = 6, fig.width = 8, fig.align = "center", warning=FALSE,  echo = FALSE}

circshift <- function(v, n) {
if (n == 0) v else c(tail(v, n), head(v, -n))
}

transpose_pitch = function(pitch_list, number_of_semitones) { # n = 1: C -> C#
names = names(pitch_list)
new_list = setNames(circshift(unname(pitch_list), number_of_semitones), names)
new_list
}

transpose_pitches = function(df, n) { # n = 1 is C -> C#
df %>% dplyr::mutate(pitches = purrr::map2(pitches, n, transpose_pitch))
}

# E min
DTW_cover <-
  get_tidy_audio_analysis("1ZczOoLuCyDO5dKUPndxf5") %>%
  select(segments) %>%
  unnest(segments) %>%
  select(start, duration, pitches)

DTW_cover_transposed = transpose_pitches(DTW_cover, 2)

# A maj - F# min
DTW_original <-
  get_tidy_audio_analysis("3NfxSdJnVdon1axzloJgba") %>%
  select(segments) %>%
  unnest(segments) %>%
  select(start, duration, pitches)

compmus_long_distance(
  DTW_original %>% mutate(pitches = map(pitches, compmus_normalise, "chebyshev")),
  DTW_cover_transposed %>% mutate(pitches = map(pitches, compmus_normalise, "chebyshev")),
  feature = pitches,
  method = "euclidean"
) %>%
  ggplot(
    aes(
      x = xstart + xduration / 2,
      width = xduration,
      y = ystart + yduration / 2,
      height = yduration,
      fill = d
    )
  ) +
  geom_tile() +
  coord_equal() +
  labs(x = "Cover", y = "Original") +
  theme_minimal() +
  scale_fill_viridis_c(guide = NULL) + theme_ilo() + theme_tufte()
```

*** 

Another interesting point to see if (and how) the generations differ from eachother, is to look at the cover of a song of an artist from the old generation, by an artist of the new generation.
In the dynamic time warping graph, *Say a little prayer* by *Aretha Franklin* is compared to the *cover* of *Lianne de Havas*. To compare the two songs, they need to be in the same key, as *Dynamic time warping* compares pith information. The cover was in Emin and the original in F#min, so the cover was transposed 2 semitones to F#min as well.

As we can see in the visualization there is no line that shows similarities. This is interesting because it is a cover of the same song, compared in the same key. The absence of similarities can have multiple reasons. The first is that the instrumentals are very different. In the cover, the guitar is very upfront and it played with a lot of vibrato, which could throw the pitch information off. Besides that, the cover is a live performance and is sang in a different way than the original (which is not a live performance). It is interesting to see that the songs do not compare and all, which could mean that the new generation of soul has a very different way of expressing the genre than the old generation. However this is **very** generalized, as we only compare one cover here.

***

<iframe style="border-radius:12px" src="https://open.spotify.com/embed/track/3NfxSdJnVdon1axzloJgba?utm_source=generator" width="100%" height="80" frameBorder="0" allowfullscreen="" allow="autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture"></iframe>

<iframe style="border-radius:12px" src="https://open.spotify.com/embed/track/1ZczOoLuCyDO5dKUPndxf5?utm_source=generator" width="100%" height="80" frameBorder="0" allowfullscreen="" allow="autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture"></iframe>

### Conclusion and Discussion

**Conclusion**

to be implemented

**Discussion**

In the portfolio, the old and new generation of soul artists exist of 6 (arbitrarily chosen) artists. While this gives some representation of how the generations compare, for further research it is adviced to use larger artist groups, with artists chosen carefully. Another point is that the comparison of the cover ALS GETRAINSPOSED NOG FIXEN!!!!!!!!!!!!!!!!!!, only consists of 1 cover. This is a very small number of comparisons, and needs to be bigger in future research.

### **chromogram, keyogram and chordogram** of an interesting soul track

```{r fig.height = 6, fig.width = 8, fig.align = "center", warning=FALSE,  echo = FALSE}

sunnyp <-
  get_tidy_audio_analysis("6IFSPx3lqkw0ri4OJkTkLl") %>%
  select(segments) %>%
  unnest(segments) %>%
  select(start, duration, pitches)


sunny <-
  sunnyp %>%
  mutate(pitches = map(pitches, compmus_normalise, "chebyshev")) %>%
  compmus_gather_chroma() %>% 
  ggplot(
    aes(
      x = start + duration / 2,
      width = duration,
      y = pitch_class,
      fill = value
    )
  ) +
  geom_tile() +
  labs(x = "Time (s)", y = NULL, fill = "Magnitude") +
  theme_minimal() +
  scale_fill_viridis_c()

circshift <- function(v, n) {
  if (n == 0) v else c(tail(v, n), head(v, -n))
}

#      C     C#    D     Eb    E     F     F#    G     Ab    A     Bb    B
major_chord <-
  c(   1,    0,    0,    0,    1,    0,    0,    1,    0,    0,    0,    0)
minor_chord <-
  c(   1,    0,    0,    1,    0,    0,    0,    1,    0,    0,    0,    0)
seventh_chord <-
  c(   1,    0,    0,    0,    1,    0,    0,    1,    0,    0,    1,    0)

major_key <-
  c(6.35, 2.23, 3.48, 2.33, 4.38, 4.09, 2.52, 5.19, 2.39, 3.66, 2.29, 2.88)
minor_key <-
  c(6.33, 2.68, 3.52, 5.38, 2.60, 3.53, 2.54, 4.75, 3.98, 2.69, 3.34, 3.17)



chord_templates <-
  tribble(
    ~name, ~template,
    "Gb:7", circshift(seventh_chord, 6),
    "Gb:maj", circshift(major_chord, 6),
    "Bb:min", circshift(minor_chord, 10),
    "Db:maj", circshift(major_chord, 1),
    "F:min", circshift(minor_chord, 5),
    "Ab:7", circshift(seventh_chord, 8),
    "Ab:maj", circshift(major_chord, 8),
    "C:min", circshift(minor_chord, 0),
    "Eb:7", circshift(seventh_chord, 3),
    "Eb:maj", circshift(major_chord, 3),
    "G:min", circshift(minor_chord, 7),
    "Bb:7", circshift(seventh_chord, 10),
    "Bb:maj", circshift(major_chord, 10),
    "D:min", circshift(minor_chord, 2),
    "F:7", circshift(seventh_chord, 5),
    "F:maj", circshift(major_chord, 5),
    "A:min", circshift(minor_chord, 9),
    "C:7", circshift(seventh_chord, 0),
    "C:maj", circshift(major_chord, 0),
    "E:min", circshift(minor_chord, 4),
    "G:7", circshift(seventh_chord, 7),
    "G:maj", circshift(major_chord, 7),
    "B:min", circshift(minor_chord, 11),
    "D:7", circshift(seventh_chord, 2),
    "D:maj", circshift(major_chord, 2),
    "F#:min", circshift(minor_chord, 6),
    "A:7", circshift(seventh_chord, 9),
    "A:maj", circshift(major_chord, 9),
    "C#:min", circshift(minor_chord, 1),
    "E:7", circshift(seventh_chord, 4),
    "E:maj", circshift(major_chord, 4),
    "G#:min", circshift(minor_chord, 8),
    "B:7", circshift(seventh_chord, 11),
    "B:maj", circshift(major_chord, 11),
    "D#:min", circshift(minor_chord, 3)
  )

key_templates <-
  tribble(
    ~name, ~template,
    "Gb:maj", circshift(major_key, 6),
    "Bb:min", circshift(minor_key, 10),
    "Db:maj", circshift(major_key, 1),
    "F:min", circshift(minor_key, 5),
    "Ab:maj", circshift(major_key, 8),
    "C:min", circshift(minor_key, 0),
    "Eb:maj", circshift(major_key, 3),
    "G:min", circshift(minor_key, 7),
    "Bb:maj", circshift(major_key, 10),
    "D:min", circshift(minor_key, 2),
    "F:maj", circshift(major_key, 5),
    "A:min", circshift(minor_key, 9),
    "C:maj", circshift(major_key, 0),
    "E:min", circshift(minor_key, 4),
    "G:maj", circshift(major_key, 7),
    "B:min", circshift(minor_key, 11),
    "D:maj", circshift(major_key, 2),
    "F#:min", circshift(minor_key, 6),
    "A:maj", circshift(major_key, 9),
    "C#:min", circshift(minor_key, 1),
    "E:maj", circshift(major_key, 4),
    "G#:min", circshift(minor_key, 8),
    "B:maj", circshift(major_key, 11),
    "D#:min", circshift(minor_key, 3)
  )

sunny10 <-
  get_tidy_audio_analysis("6IFSPx3lqkw0ri4OJkTkLl") %>%
  compmus_align(sections, segments) %>%
  select(sections) %>%
  unnest(sections) %>%
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "mean", norm = "manhattan"
      )
  )

sunny1 <-
  sunny10 %>% 
  compmus_match_pitch_template(
    key_templates,         # Change to chord_templates if descired
    method = "euclidean",  # Try different distance metrics
    norm = "manhattan"     # Try different norms
  ) %>%
  ggplot(
    aes(x = start + duration / 2, width = duration, y = name, fill = d)
  ) +
  geom_tile() +
  scale_fill_viridis_c(guide = "none") +
  theme_minimal() +
  labs(x = "Time (s)", y = "")

sunny2 <-
  sunny10 %>% 
  compmus_match_pitch_template(
    chord_templates,         # Change to chord_templates if descired
    method = "euclidean",  # Try different distance metrics
    norm = "manhattan"     # Try different norms
  ) %>%
  ggplot(
    aes(x = start + duration / 2, width = duration, y = name, fill = d)
  ) +
  geom_tile() +
  scale_fill_viridis_c(guide = "none") +
  theme_minimal() +
  labs(x = "Time (s)", y = "")

grid.arrange(sunny1, sunny, sunny2, ncol = 2, widths = 1:2)
#grid.arrange(sunny1, sunny2, ncol = 2, widths = 1:2)
```

***

I decided to visualize the song *"Sunny"* by *Marvin gaye* (the original is from Bobby Hebbs).
I really wanted to visualize this song, because it is a great song in itself **transposes** a few times. As seen in the chromogram, the song starts in *Fmin* transposes to *F#min*, *Gmin* and ends in *G#min*. We can see this in our keyogram and chordogram visualized as well.
I think this song is very interesting to see in the visualization, as you can "see the transposing happen". 

***

<iframe style="border-radius:12px" src="https://open.spotify.com/embed/track/6IFSPx3lqkw0ri4OJkTkLl?utm_source=generator&theme=0" width="100%" height="80" frameBorder="0" allowfullscreen="" allow="autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture"></iframe>

<iframe style="border-radius:12px" src="https://open.spotify.com/embed/track/4vju55Ag7apDL2CfotuE7Q?utm_source=generator" width="100%" height="80" frameBorder="0" allowfullscreen="" allow="autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture"></iframe>

### **Self-Similarity Matrices** for a song of **Tom Misch & FKJ**

```{r fig.height = 6, fig.width = 8, fig.align = "center", warning=FALSE,  echo = FALSE}
tom_misch_self_similar <-   
  get_tidy_audio_analysis("4yMT3mbUsiukLDRqPfQ9rN") %>% # Change URI.
  compmus_align(bars, segments) %>%                     # Change `bars`
  select(bars) %>%                                      #   in all three
  unnest(bars) %>%                                      #   of these lines.
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "rms", norm = "euclidean"              # Change summary & norm.
      )
  ) %>%
  mutate(
    timbre =
      map(segments,
        compmus_summarise, timbre,
        method = "rms", norm = "euclidean"              # Change summary & norm.
      )
  )

SS1 <- tom_misch_self_similar %>%
  compmus_self_similarity(timbre, "cosine") %>% 
  ggplot(
    aes(
      x = xstart + xduration / 2,
      width = xduration,
      y = ystart + yduration / 2,
      height = yduration,
      fill = d
    )
  ) +
  geom_tile() +
  coord_fixed() +
  scale_fill_viridis_c(guide = "none") +
  theme_classic() +
  labs(x = "", y = "")  + ggtitle("Timbre of \"losing my way\" - sections") + theme_ilo() + theme_tufte()

SS2 <- tom_misch_self_similar %>%
  compmus_self_similarity(pitches, "cosine") %>% 
  ggplot(
    aes(
      x = xstart + xduration / 2,
      width = xduration,
      y = ystart + yduration / 2,
      height = yduration,
      fill = d
    )
  ) +
  geom_tile() +
  coord_fixed() +
  scale_fill_viridis_c(guide = "none") +
  theme_classic() +
  labs(x = "", y = "") + ggtitle("chroma of \"losing my way\" - sections") +theme_ilo() + theme_tufte()

grid.arrange(SS1, SS2, ncol = 1)

```

*** 

to understand how the structure of a song works, a song of the new generation is visualized. The song is made by 2 artist from the new generation, making it more representable for the genre than other songs. In the Self-Similarity matrix, the song *Losing my way* from *Tom Misch & FKJ* is analized. First we analize via a timbre SSM. **Timbre** can be described as the feel of a track, and what different instruments are used, but the term is hard to describe. As the timbre SS matrix shows, there are different sections in the song that repeated. The sections are as A - B - C - (B+C) - D. As we can see in the chroma SS matrix, different segments of the song have different pitch characteristics. Some repetitions are high in pitch and others are low. light means the pitch characteristics are way different there then in the rest of the song. This is the same for the end. the rest of the pattern follows the sections, as explained above.

***

<iframe style="border-radius:12px" src="https://open.spotify.com/embed/track/4yMT3mbUsiukLDRqPfQ9rN?utm_source=generator" width="100%" height="80" frameBorder="0" allowfullscreen="" allow="autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture"></iframe>

